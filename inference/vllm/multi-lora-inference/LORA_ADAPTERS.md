# åˆ©ç”¨å¯èƒ½ãªLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼

vLLM Multi-LoRAæ¨è«–ã§ä½¿ç”¨ã§ãã‚‹ã€å…¬é–‹ã•ã‚Œã¦ã„ã‚‹LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒªã‚¹ãƒˆã§ã™ã€‚

## TinyLlama (1.1B) - æœ€è»½é‡

### é–¢æ•°å‘¼ã³å‡ºã—
- **ãƒ¢ãƒ‡ãƒ«**: `unclecode/tinyllama-function-call-lora-adapter-250424`
- **ç”¨é€”**: é–¢æ•°å‘¼ã³å‡ºã—ã€ãƒ„ãƒ¼ãƒ«åˆ©ç”¨
- **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: TinyLlama/TinyLlama-1.1B-Chat-v1.0
- **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: Apache 2.0
- **ä½¿ç”¨ä¾‹**: APIå‘¼ã³å‡ºã—ã€ãƒ„ãƒ¼ãƒ«ã®å‹•çš„å®Ÿè¡Œ

### SQLç”Ÿæˆ
- **ãƒ¢ãƒ‡ãƒ«**: `sid321axn/tiny-llama-text2sql`
- **ç”¨é€”**: è‡ªç„¶è¨€èªã‹ã‚‰SQLã‚¯ã‚¨ãƒªç”Ÿæˆ
- **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: TinyLlama/TinyLlama-1.1B-Chat-v0.3
- **ä½¿ç”¨ä¾‹**: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã®è‡ªå‹•ç”Ÿæˆ

### æ•°å­¦å•é¡Œè§£ç­”
- **ãƒ¢ãƒ‡ãƒ«**: `philimon/TinyLlama-gsm8k-lora`
- **ç”¨é€”**: GSM8Kæ•°å­¦å•é¡Œã®è§£ç­”
- **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: TinyLlama/TinyLlama-1.1B-Chat-v0.3
- **ä½¿ç”¨ä¾‹**: ç®—æ•°ãƒ»æ•°å­¦ã®æ–‡ç« å•é¡Œ

### ãã®ä»–
- `lightblue/tinyllama_chat_jsquad` - ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
- `rocailler/tinyllama-1b-20K-ProdToCat-2023-11-26_00-10` - ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
- `tushkulange/tinyllama-text-to-sql-lora` - Text-to-SQLï¼ˆåˆ¥å®Ÿè£…ï¼‰
- `EddyGiusepe/tinyllama-ItauPortuguese-lora-v0.1` - ãƒãƒ«ãƒˆã‚¬ãƒ«èªãƒãƒ£ãƒƒãƒˆ

**åˆè¨ˆ**: 36å€‹ä»¥ä¸Šã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼åˆ©ç”¨å¯èƒ½

## Microsoft Phi-2 (2.7B) - ãƒãƒ©ãƒ³ã‚¹å‹

### ã‚«ã‚¹ã‚¿ãƒ QLoRA
- **ãƒ¢ãƒ‡ãƒ«**: `piyushgrover/phi-2-qlora-adapter-custom`
- **ç”¨é€”**: ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¹ã‚¯ç”¨QLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼

### åŒ»ç™‚ãƒ†ã‚­ã‚¹ãƒˆ
- **ãƒ¢ãƒ‡ãƒ«**: `NouRed/Med-Phi-2-QLoRa`
- **ç”¨é€”**: åŒ»ç™‚ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ»ç†è§£

### å¤šè¨€èªå¯¾å¿œ
- **ãƒ¢ãƒ‡ãƒ«**: `s3nh/phi-2_dolly_instruction_polish_adapter`
- **ç”¨é€”**: ãƒãƒ¼ãƒ©ãƒ³ãƒ‰èªå‘½ä»¤ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### å‘½ä»¤ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- **ãƒ¢ãƒ‡ãƒ«**: `Yhyu13/phi-2-sft-alpaca_gpt4_en-ep1-lora`
- **ç”¨é€”**: Alpaca/GPT4ã‚¹ã‚¿ã‚¤ãƒ«ã®å‘½ä»¤å®Ÿè¡Œ

**åˆè¨ˆ**: 948å€‹ä»¥ä¸Šã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼åˆ©ç”¨å¯èƒ½

## Qwen2.5-7B (7B) - é«˜æ€§èƒ½

### ä¸­å›½èªãƒ†ã‚­ã‚¹ãƒˆä¿®æ­£
- **ãƒ¢ãƒ‡ãƒ«**: `shibing624/chinese-text-correction-7b-lora`
- **ç”¨é€”**: ä¸­å›½èªã®æ–‡æ³•ãƒ»ã‚¹ãƒšãƒ«ä¿®æ­£
- **äººæ°—åº¦**: â­â­

### æ•°å­¦å•é¡Œè§£ç­”
- **ãƒ¢ãƒ‡ãƒ«**: `ybian-umd/Qwen2.5-7B-Instruct-gsm8k-*`
- **ç”¨é€”**: GSM8Kæ•°å­¦å•é¡Œï¼ˆè¤‡æ•°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰
- **äººæ°—åº¦**: â­â­â­

### ORPOæœ€é©åŒ–
- **ãƒ¢ãƒ‡ãƒ«**: `FINGU-AI/Qwen2.5-orpo-lora`
- **ç”¨é€”**: ORPOï¼ˆOdds Ratio Preference Optimizationï¼‰
- **äººæ°—åº¦**: â­â­â­

### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
- **ãƒ¢ãƒ‡ãƒ«**: `vlkn/FT-Qwen2.5-7B-GlossLM`
- **ç”¨é€”**: æ±ç”¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ

**åˆè¨ˆ**: 859å€‹ä»¥ä¸Šã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼åˆ©ç”¨å¯èƒ½

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬çš„ãªä½¿ã„æ–¹

```python
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
llm = LLM(
    model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    enable_lora=True,
    max_loras=3,
    max_lora_rank=64,
)

# LoRAãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½œæˆ
# lora_pathã«HuggingFaceãƒªãƒã‚¸ãƒˆãƒªIDã‚’æŒ‡å®šã™ã‚‹ã¨è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™
lora_request = LoRARequest(
    lora_name="function_call",
    lora_int_id=1,
    lora_path="unclecode/tinyllama-function-call-lora-adapter-250424",
)

# æ¨è«–å®Ÿè¡Œ
outputs = llm.generate(
    prompts=["Your prompt here"],
    sampling_params=SamplingParams(temperature=0.7, max_tokens=200),
    lora_request=lora_request,
)
```

### è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹

```python
# è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’å®šç¾©
# ç¬¬1å¼•æ•°: lora_name, ç¬¬2å¼•æ•°: lora_int_id, ç¬¬3å¼•æ•°: lora_path
adapters = {
    "sql": LoRARequest(lora_name="sql", lora_int_id=1, lora_path="sid321axn/tiny-llama-text2sql"),
    "math": LoRARequest(lora_name="math", lora_int_id=2, lora_path="philimon/TinyLlama-gsm8k-lora"),
    "function": LoRARequest(lora_name="function", lora_int_id=3, lora_path="unclecode/tinyllama-function-call-lora-adapter-250424"),
}

# ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦åˆ‡ã‚Šæ›¿ãˆ
outputs1 = llm.generate(["Generate SQL..."], lora_request=adapters["sql"])
outputs2 = llm.generate(["Solve: 2+2=?"], lora_request=adapters["math"])
outputs3 = llm.generate(["Create function..."], lora_request=adapters["function"])
```

## ãƒ¢ãƒ‡ãƒ«é¸æŠã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

| ãƒ¢ãƒ‡ãƒ« | ãƒ¡ãƒ¢ãƒª | é€Ÿåº¦ | å“è³ª | æ¨å¥¨ç”¨é€” |
|--------|--------|------|------|----------|
| TinyLlama (1.1B) | ğŸŸ¢ æœ€å° | ğŸŸ¢ æœ€é€Ÿ | ğŸŸ¡ ä¸­ | ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç† |
| Phi-2 (2.7B) | ğŸŸ¡ å° | ğŸŸ¢ é«˜é€Ÿ | ğŸŸ¢ é«˜ | ãƒãƒ©ãƒ³ã‚¹å‹ã€æœ¬ç•ªç’°å¢ƒ |
| Qwen2.5-7B (7B) | ğŸŸ  ä¸­ | ğŸŸ¡ ä¸­é€Ÿ | ğŸŸ¢ æœ€é«˜ | é«˜å“è³ªãŒå¿…è¦ãªå ´åˆ |

## æ³¨æ„äº‹é …

1. **äº’æ›æ€§**: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯ç‰¹å®šã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ã€‚ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã¯å‹•ä½œã—ã¾ã›ã‚“ã€‚
2. **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: å„LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
3. **ãƒ¡ãƒ¢ãƒª**: è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’åŒæ™‚ã«ä½¿ç”¨ã™ã‚‹å ´åˆã€`max_loras`ã¨`max_cpu_loras`ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚
4. **rank**: ä½¿ç”¨ã™ã‚‹LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®rankã‚’ç¢ºèªã—ã€`max_lora_rank`ã‚’é©åˆ‡ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚

## ã•ã‚‰ã«æ¢ã™

- [Hugging Face - TinyLlama ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼](https://huggingface.co/models?other=base_model:adapter:TinyLlama/TinyLlama-1.1B-Chat-v0.3)
- [Hugging Face - Phi-2 ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼](https://huggingface.co/models?other=base_model:adapter:microsoft/phi-2)
- [Hugging Face - Qwen2.5-7B ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼](https://huggingface.co/models?other=base_model:adapter:Qwen/Qwen2.5-7B-Instruct)
