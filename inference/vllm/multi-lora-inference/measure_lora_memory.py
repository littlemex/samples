#!/usr/bin/env python3
"""
LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸¬å®šã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’æ¸¬å®šã—ã¾ã™ï¼š
1. å„LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ‡ã‚£ã‚¹ã‚¯ã‚µã‚¤ã‚º
2. LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã¨ãã®CPU RAMã¨GPU RAMã®ä½¿ç”¨é‡
3. max_cpu_lorasãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿

ä½¿ç”¨ä¾‹:
  python measure_lora_memory.py
  python measure_lora_memory.py --max-cpu-loras 10
  python measure_lora_memory.py --output lora_memory_report.txt
"""

import argparse
import os
import psutil
import subprocess
import time
from pathlib import Path
from typing import Dict, List

from huggingface_hub import snapshot_download
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest


# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®å®šç¾©
LORA_ADAPTERS = [
    {
        "name": "text2sql",
        "path": "sid321axn/tiny-llama-text2sql",
        "description": "SQLç”ŸæˆLoRA",
    },
    {
        "name": "math",
        "path": "philimon/TinyLlama-gsm8k-lora",
        "description": "æ•°å­¦å•é¡ŒLoRA",
    },
    {
        "name": "function",
        "path": "unclecode/tinyllama-function-call-lora-adapter-250424",
        "description": "é–¢æ•°å‘¼ã³å‡ºã—LoRA",
    },
]


def get_gpu_memory_mb() -> float:
    """nvidia-smiã‚’ä½¿ã£ã¦GPUå…¨ä½“ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],
            capture_output=True,
            text=True,
            check=True
        )
        memory_mb = float(result.stdout.strip().split('\n')[0])
        return memory_mb
    except Exception:
        return 0.0


def get_cpu_memory_mb() -> Dict[str, float]:
    """CPU RAMã®ä½¿ç”¨é‡ã‚’å–å¾—"""
    memory = psutil.virtual_memory()
    return {
        "total": memory.total / 1024 / 1024,
        "available": memory.available / 1024 / 1024,
        "used": memory.used / 1024 / 1024,
        "percent": memory.percent
    }


def get_process_memory_mb() -> float:
    """ç¾åœ¨ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024


def get_lora_disk_size(lora_path: str) -> Dict[str, float]:
    """LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ‡ã‚£ã‚¹ã‚¯ã‚µã‚¤ã‚ºã‚’å–å¾—"""
    try:
        # HuggingFaceã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä½¿ç”¨ï¼‰
        cache_dir = snapshot_download(lora_path)

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’åˆè¨ˆ
        total_size = 0
        file_sizes = {}

        for root, dirs, files in os.walk(cache_dir):
            for file in files:
                file_path = os.path.join(root, file)
                size = os.path.getsize(file_path)
                total_size += size
                file_sizes[file] = size / 1024 / 1024  # MB

        return {
            "total_mb": total_size / 1024 / 1024,
            "cache_dir": cache_dir,
            "files": file_sizes
        }
    except Exception as e:
        return {"error": str(e), "total_mb": 0}


def measure_lora_loading_memory(
    model_name: str,
    lora_adapters: List[Dict],
    max_cpu_loras: int
) -> Dict:
    """LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ­ãƒ¼ãƒ‰æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸¬å®š"""

    print(f"\n{'='*80}")
    print(f"ğŸ“Š LoRAãƒ­ãƒ¼ãƒ‰æ™‚ã®ãƒ¡ãƒ¢ãƒªæ¸¬å®š (max_cpu_loras={max_cpu_loras})")
    print(f"{'='*80}")

    # åˆæœŸçŠ¶æ…‹ã®ãƒ¡ãƒ¢ãƒª
    cpu_mem_before = get_cpu_memory_mb()
    gpu_mem_before = get_gpu_memory_mb()
    process_mem_before = get_process_memory_mb()

    print(f"\nåˆæœŸçŠ¶æ…‹:")
    print(f"  CPU RAMä½¿ç”¨é‡: {cpu_mem_before['used']:.2f} MB ({cpu_mem_before['percent']:.1f}%)")
    print(f"  ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒª: {process_mem_before:.2f} MB")
    print(f"  GPU ãƒ¡ãƒ¢ãƒª: {gpu_mem_before:.2f} MB")

    # LLMåˆæœŸåŒ–
    print(f"\nğŸš€ Multi-LoRAãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
    llm = LLM(
        model=model_name,
        enable_lora=True,
        max_loras=len(lora_adapters),
        max_lora_rank=64,
        max_cpu_loras=max_cpu_loras,
        gpu_memory_utilization=0.85,
    )

    cpu_mem_after_init = get_cpu_memory_mb()
    gpu_mem_after_init = get_gpu_memory_mb()
    process_mem_after_init = get_process_memory_mb()

    print(f"âœ… ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
    print(f"  CPU RAMå¢—åŠ : {cpu_mem_after_init['used'] - cpu_mem_before['used']:.2f} MB")
    print(f"  ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒªå¢—åŠ : {process_mem_after_init - process_mem_before:.2f} MB")
    print(f"  GPU ãƒ¡ãƒ¢ãƒªå¢—åŠ : {gpu_mem_after_init - gpu_mem_before:.2f} MB")

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    sampling_params = SamplingParams(temperature=0.0, max_tokens=50)

    # å„LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
    print(f"\nğŸ”„ {len(lora_adapters)}å€‹ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    lora_memory_usage = []

    for i, adapter in enumerate(lora_adapters, 1):
        print(f"\n  {i}. {adapter['name']} ã‚’ãƒ­ãƒ¼ãƒ‰...")

        cpu_mem_before_lora = get_cpu_memory_mb()
        process_mem_before_lora = get_process_memory_mb()

        lora_request = LoRARequest(
            lora_name=adapter['name'],
            lora_int_id=i,
            lora_path=adapter['path'],
        )

        # LoRAã‚’å®Ÿéš›ã«ä½¿ç”¨ã—ã¦å¼·åˆ¶çš„ã«ãƒ­ãƒ¼ãƒ‰
        _ = llm.generate(
            prompts=["<|user|>\nTest</s>\n<|assistant|>\n"],
            sampling_params=sampling_params,
            lora_request=lora_request,
        )

        cpu_mem_after_lora = get_cpu_memory_mb()
        process_mem_after_lora = get_process_memory_mb()

        cpu_increase = cpu_mem_after_lora['used'] - cpu_mem_before_lora['used']
        process_increase = process_mem_after_lora - process_mem_before_lora

        print(f"     CPU RAMå¢—åŠ : {cpu_increase:.2f} MB")
        print(f"     ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒªå¢—åŠ : {process_increase:.2f} MB")

        lora_memory_usage.append({
            "name": adapter['name'],
            "cpu_increase_mb": cpu_increase,
            "process_increase_mb": process_increase,
        })

    # æœ€çµ‚çŠ¶æ…‹
    cpu_mem_final = get_cpu_memory_mb()
    gpu_mem_final = get_gpu_memory_mb()
    process_mem_final = get_process_memory_mb()

    print(f"\næœ€çµ‚çŠ¶æ…‹:")
    print(f"  CPU RAMä½¿ç”¨é‡: {cpu_mem_final['used']:.2f} MB ({cpu_mem_final['percent']:.1f}%)")
    print(f"  ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒª: {process_mem_final:.2f} MB")
    print(f"  GPU ãƒ¡ãƒ¢ãƒª: {gpu_mem_final:.2f} MB")

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    del llm
    time.sleep(2)

    return {
        "max_cpu_loras": max_cpu_loras,
        "cpu_ram_before": cpu_mem_before,
        "cpu_ram_after": cpu_mem_final,
        "cpu_ram_increase": cpu_mem_final['used'] - cpu_mem_before['used'],
        "process_memory_before": process_mem_before,
        "process_memory_after": process_mem_final,
        "process_memory_increase": process_mem_final - process_mem_before,
        "gpu_memory_before": gpu_mem_before,
        "gpu_memory_after": gpu_mem_final,
        "gpu_memory_increase": gpu_mem_final - gpu_mem_before,
        "lora_memory_usage": lora_memory_usage,
    }


def main():
    parser = argparse.ArgumentParser(
        description="LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸¬å®š"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        help="ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«",
    )
    parser.add_argument(
        "--max-cpu-loras",
        type=int,
        default=None,
        help="CPUå´ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹LoRAæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: max_lorasã¨åŒã˜ï¼‰",
    )
    parser.add_argument(
        "--output",
        type=Path,
        help="çµæœã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«",
    )

    args = parser.parse_args()

    print("="*80)
    print("ğŸ”¬ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ ãƒ¡ãƒ¢ãƒªæ¸¬å®š")
    print("="*80)
    print(f"ãƒ¢ãƒ‡ãƒ«: {args.model}")
    print(f"æ¸¬å®šå¯¾è±¡LoRAæ•°: {len(LORA_ADAPTERS)}")

    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    cpu_mem = get_cpu_memory_mb()
    print(f"\nã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
    print(f"  CPU RAMç·å®¹é‡: {cpu_mem['total']:.2f} MB ({cpu_mem['total']/1024:.2f} GB)")
    print(f"  CPU RAMä½¿ç”¨å¯èƒ½: {cpu_mem['available']:.2f} MB ({cpu_mem['available']/1024:.2f} GB)")

    # ã‚¹ãƒ†ãƒƒãƒ—1: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ‡ã‚£ã‚¹ã‚¯ã‚µã‚¤ã‚ºã‚’æ¸¬å®š
    print(f"\n{'='*80}")
    print("ğŸ“‚ ã‚¹ãƒ†ãƒƒãƒ—1: LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ‡ã‚£ã‚¹ã‚¯ã‚µã‚¤ã‚º")
    print(f"{'='*80}")

    lora_disk_sizes = []
    for adapter in LORA_ADAPTERS:
        print(f"\n{adapter['name']} ({adapter['description']}):")
        size_info = get_lora_disk_size(adapter['path'])

        if "error" not in size_info:
            print(f"  ç·ã‚µã‚¤ã‚º: {size_info['total_mb']:.2f} MB")
            print(f"  ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {size_info['cache_dir']}")
            print(f"  ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«:")
            for file, size in sorted(size_info['files'].items(), key=lambda x: -x[1])[:3]:
                print(f"    - {file}: {size:.2f} MB")

            lora_disk_sizes.append({
                "name": adapter['name'],
                "size_mb": size_info['total_mb'],
            })
        else:
            print(f"  ã‚¨ãƒ©ãƒ¼: {size_info['error']}")

    total_disk_size = sum(item['size_mb'] for item in lora_disk_sizes)
    print(f"\nğŸ“Š LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼åˆè¨ˆãƒ‡ã‚£ã‚¹ã‚¯ã‚µã‚¤ã‚º: {total_disk_size:.2f} MB ({total_disk_size/1024:.2f} GB)")

    # ã‚¹ãƒ†ãƒƒãƒ—2: LoRAãƒ­ãƒ¼ãƒ‰æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸¬å®š
    max_cpu_loras = args.max_cpu_loras if args.max_cpu_loras else len(LORA_ADAPTERS)
    memory_result = measure_lora_loading_memory(args.model, LORA_ADAPTERS, max_cpu_loras)

    # ã‚µãƒãƒªãƒ¼
    print(f"\n{'='*80}")
    print("ğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚µãƒãƒªãƒ¼")
    print(f"{'='*80}")

    print(f"\nã€ãƒ‡ã‚£ã‚¹ã‚¯ã‚µã‚¤ã‚ºã€‘")
    for item in lora_disk_sizes:
        print(f"  {item['name']}: {item['size_mb']:.2f} MB")
    print(f"  åˆè¨ˆ: {total_disk_size:.2f} MB")

    print(f"\nã€ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãƒ¡ãƒ¢ãƒªã€‘")
    print(f"  ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒªå¢—åŠ : {memory_result['process_memory_increase']:.2f} MB")
    print(f"  CPU RAMå¢—åŠ : {memory_result['cpu_ram_increase']:.2f} MB")
    print(f"  GPU ãƒ¡ãƒ¢ãƒªå¢—åŠ : {memory_result['gpu_memory_increase']:.2f} MB")

    print(f"\nã€å„LoRAãƒ­ãƒ¼ãƒ‰æ™‚ã®å¢—åŠ é‡ã€‘")
    for lora in memory_result['lora_memory_usage']:
        print(f"  {lora['name']}:")
        print(f"    CPU RAM: +{lora['cpu_increase_mb']:.2f} MB")
        print(f"    ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒª: +{lora['process_increase_mb']:.2f} MB")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)

        with open(args.output, 'w', encoding='utf-8') as f:
            f.write("LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ ãƒ¡ãƒ¢ãƒªæ¸¬å®šçµæœ\n")
            f.write("="*80 + "\n\n")
            f.write(f"ãƒ¢ãƒ‡ãƒ«: {args.model}\n")
            f.write(f"max_cpu_loras: {max_cpu_loras}\n\n")

            f.write("ãƒ‡ã‚£ã‚¹ã‚¯ã‚µã‚¤ã‚º:\n")
            for item in lora_disk_sizes:
                f.write(f"  {item['name']}: {item['size_mb']:.2f} MB\n")
            f.write(f"  åˆè¨ˆ: {total_disk_size:.2f} MB\n\n")

            f.write("ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãƒ¡ãƒ¢ãƒª:\n")
            f.write(f"  ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒªå¢—åŠ : {memory_result['process_memory_increase']:.2f} MB\n")
            f.write(f"  CPU RAMå¢—åŠ : {memory_result['cpu_ram_increase']:.2f} MB\n")
            f.write(f"  GPU ãƒ¡ãƒ¢ãƒªå¢—åŠ : {memory_result['gpu_memory_increase']:.2f} MB\n\n")

            f.write("å„LoRAãƒ­ãƒ¼ãƒ‰æ™‚ã®å¢—åŠ é‡:\n")
            for lora in memory_result['lora_memory_usage']:
                f.write(f"  {lora['name']}:\n")
                f.write(f"    CPU RAM: +{lora['cpu_increase_mb']:.2f} MB\n")
                f.write(f"    ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒª: +{lora['process_increase_mb']:.2f} MB\n")

        print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜: {args.output}")

    print("\nâœ… ãƒ¡ãƒ¢ãƒªæ¸¬å®šå®Œäº†ï¼")

    # æ³¨æ„äº‹é …ã‚’è¡¨ç¤º
    print(f"\n{'='*80}")
    print("âš ï¸  é‡è¦ãªæ³¨æ„äº‹é …")
    print(f"{'='*80}")
    print(f"""
1. CPU RAMå®¹é‡ã®è€ƒæ…®:
   - ç¾åœ¨ã®ç©ºãå®¹é‡: {cpu_mem['available']/1024:.2f} GB
   - max_cpu_lorasãŒå¤§ãã„ã¨ã€å¤šæ•°ã®LoRAãŒCPU RAMã«ä¿æŒã•ã‚Œã¾ã™
   - æ¨å¥¨: ç©ºãRAMã®50%ä»¥ä¸‹ã«æŠ‘ãˆã‚‹

2. LoRA 1å€‹ã‚ãŸã‚Šã®æ¨å®šã‚µã‚¤ã‚º:
   - ãƒ‡ã‚£ã‚¹ã‚¯ã‚µã‚¤ã‚º: ç´„{total_disk_size/len(LORA_ADAPTERS):.2f} MB
   - ãƒ©ãƒ³ã‚¿ã‚¤ãƒ å¢—åŠ : ç´„{memory_result['process_memory_increase']/len(LORA_ADAPTERS):.2f} MB

3. max_cpu_lorasã®æ¨å¥¨å€¤:
   - ç¾åœ¨ã®è¨­å®š: {max_cpu_loras}å€‹
   - ç†è«–çš„æœ€å¤§å€¤ï¼ˆRAM 50%ä½¿ç”¨ï¼‰: ç´„{int((cpu_mem['available']/2) / (total_disk_size/len(LORA_ADAPTERS)))}å€‹
   - å®Ÿç”¨çš„æ¨å¥¨å€¤: 10-50å€‹ï¼ˆé »ç¹ã«ä½¿ã†LoRAã®ã¿ï¼‰

4. ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥:
   - LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯ ~/.cache/huggingface/ ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™
   - ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚‚ç¢ºèªã—ã¦ãã ã•ã„
""")


if __name__ == "__main__":
    main()
