#!/usr/bin/env python3
"""
txtãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ãƒãƒƒãƒãƒ†ã‚¹ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
  python batch_test_lora.py --prompt-file test_prompts/base_model.txt
  python batch_test_lora.py --prompt-file test_prompts/sql_generation.txt --lora text2sql
"""

import argparse
from pathlib import Path
from typing import List, Optional
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest


def load_prompts(file_path: Path) -> List[str]:
    """
    txtãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã‚€

    - 1è¡Œ1ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    - #ã§å§‹ã¾ã‚‹è¡Œã¯ã‚³ãƒ¡ãƒ³ãƒˆ
    - ç©ºè¡Œã¯ç„¡è¦–
    """
    prompts = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã¨ç©ºè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if line and not line.startswith('#'):
                prompts.append(line)
    return prompts


def format_chat_prompt(user_message: str, system_message: Optional[str] = None) -> str:
    """
    TinyLlamaã®ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨

    ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
    <|system|>
    ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸</s>
    <|user|>
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸</s>
    <|assistant|>
    """
    if system_message:
        prompt = f"<|system|>\n{system_message}</s>\n<|user|>\n{user_message}</s>\n<|assistant|>\n"
    else:
        prompt = f"<|user|>\n{user_message}</s>\n<|assistant|>\n"
    return prompt


# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®å®šç¾©
LORA_ADAPTERS = {
    "text2sql": {
        "request": LoRARequest(
            lora_name="text2sql",
            lora_int_id=1,
            lora_path="sid321axn/tiny-llama-text2sql",
        ),
        "system_message": "You are a SQL expert. Generate SQL queries based on the user's request.",
    },
    "math": {
        "request": LoRARequest(
            lora_name="math",
            lora_int_id=2,
            lora_path="philimon/TinyLlama-gsm8k-lora",
        ),
        "system_message": "You are a math tutor. Solve the problem step by step.",
    },
    "function": {
        "request": LoRARequest(
            lora_name="function",
            lora_int_id=3,
            lora_path="unclecode/tinyllama-function-call-lora-adapter-250424",
        ),
        "system_message": "You are a helpful assistant that can call functions.",
    },
}


def main():
    parser = argparse.ArgumentParser(description="LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒãƒƒãƒãƒ†ã‚¹ãƒˆ")
    parser.add_argument(
        "--prompt-file",
        type=Path,
        required=True,
        help="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¨˜è¼‰ã•ã‚ŒãŸtxtãƒ•ã‚¡ã‚¤ãƒ«",
    )
    parser.add_argument(
        "--lora",
        type=str,
        choices=list(LORA_ADAPTERS.keys()) + ["none"],
        default="none",
        help="ä½¿ç”¨ã™ã‚‹LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ (none=ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿)",
    )
    parser.add_argument(
        "--model",
        type=str,
        default="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        help="ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«",
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=150,
        help="æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦",
    )
    parser.add_argument(
        "--output",
        type=Path,
        help="çµæœã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ« (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)",
    )
    args = parser.parse_args()

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã¿
    print(f"ğŸ“„ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {args.prompt_file}")
    prompts = load_prompts(args.prompt_file)
    print(f"âœ… {len(prompts)}å€‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\n")

    # LLMã‚’åˆæœŸåŒ–
    print(f"ğŸš€ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–: {args.model}")
    llm = LLM(
        model=args.model,
        enable_lora=(args.lora != "none"),
        max_loras=1,
        max_lora_rank=64,
        gpu_memory_utilization=0.85,
    )

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    sampling_params = SamplingParams(
        temperature=args.temperature,
        top_p=0.95,
        max_tokens=args.max_tokens,
    )

    # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®è¨­å®š
    lora_request = None
    system_message = "You are a helpful assistant."

    if args.lora != "none":
        adapter = LORA_ADAPTERS[args.lora]
        lora_request = adapter["request"]
        system_message = adapter["system_message"]
        print(f"ğŸ¯ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼: {args.lora}")
    else:
        print(f"ğŸ¯ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿")

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    formatted_prompts = [
        format_chat_prompt(prompt, system_message)
        for prompt in prompts
    ]

    # ãƒãƒƒãƒæ¨è«–
    print(f"\nâš™ï¸  æ¨è«–å®Ÿè¡Œä¸­...\n")
    print("=" * 80)

    outputs = llm.generate(
        prompts=formatted_prompts,
        sampling_params=sampling_params,
        lora_request=lora_request,
    )

    # çµæœã‚’è¡¨ç¤ºãƒ»ä¿å­˜
    results = []
    for i, (prompt, output) in enumerate(zip(prompts, outputs), 1):
        generated_text = output.outputs[0].text.strip()
        num_tokens = len(output.outputs[0].token_ids)

        result_text = f"""
--- ãƒ†ã‚¹ãƒˆ {i}/{len(prompts)} ---
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}

ç”Ÿæˆçµæœ:
{generated_text}

ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {num_tokens}
{'-' * 80}
"""
        print(result_text)
        results.append(result_text)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(f"ãƒ¢ãƒ‡ãƒ«: {args.model}\n")
            f.write(f"LoRA: {args.lora}\n")
            f.write(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {args.prompt_file}\n")
            f.write(f"Temperature: {args.temperature}\n")
            f.write(f"Max tokens: {args.max_tokens}\n")
            f.write("=" * 80 + "\n\n")
            f.write("\n".join(results))
        print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜: {args.output}")

    print("\nâœ… ãƒãƒƒãƒãƒ†ã‚¹ãƒˆå®Œäº†ï¼")


if __name__ == "__main__":
    main()
