#!/usr/bin/env python3
"""
Multi-LoRA servingã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›åŠ¹æœã‚’æ¸¬å®šã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ¸¬å®šå†…å®¹:
1. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿
2. å€‹åˆ¥LoRA Ã— 3ï¼ˆãã‚Œãã‚Œåˆ¥ã®LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼‰
3. Multi-LoRAï¼ˆ1ã¤ã®LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«3ã¤ã®LoRAåŒæ™‚ãƒ­ãƒ¼ãƒ‰ï¼‰

ç†è«–çš„ãªãƒ¡ãƒ¢ãƒªå‰Šæ¸›é‡ã‚’å®šé‡åŒ–ã—ã¾ã™ã€‚

ä½¿ç”¨ä¾‹:
  python measure_memory_consumption.py
  python measure_memory_consumption.py --model TinyLlama/TinyLlama-1.1B-Chat-v1.0
  python measure_memory_consumption.py --output memory_report.txt
"""

import argparse
import gc
import time
from pathlib import Path
from typing import Dict, List
import torch
from vllm import LLM
from vllm.lora.request import LoRARequest


# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®å®šç¾©
LORA_ADAPTERS = [
    {
        "name": "text2sql",
        "path": "sid321axn/tiny-llama-text2sql",
        "description": "SQLç”ŸæˆLoRA",
    },
    {
        "name": "math",
        "path": "philimon/TinyLlama-gsm8k-lora",
        "description": "æ•°å­¦å•é¡ŒLoRA",
    },
    {
        "name": "function",
        "path": "unclecode/tinyllama-function-call-lora-adapter-250424",
        "description": "é–¢æ•°å‘¼ã³å‡ºã—LoRA",
    },
]


def get_gpu_memory_mb() -> float:
    """ç¾åœ¨ã®GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’MBå˜ä½ã§å–å¾—"""
    if torch.cuda.is_available():
        return torch.cuda.memory_allocated() / 1024 / 1024
    return 0.0


def get_gpu_memory_reserved_mb() -> float:
    """GPUã«äºˆç´„ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªã‚’MBå˜ä½ã§å–å¾—"""
    if torch.cuda.is_available():
        return torch.cuda.memory_reserved() / 1024 / 1024
    return 0.0


def clear_memory():
    """ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    time.sleep(2)


def measure_base_model(model_name: str) -> Dict:
    """ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿ã®ãƒ¡ãƒ¢ãƒªæ¶ˆè²»é‡ã‚’æ¸¬å®š"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¸¬å®š 1/3: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿")
    print("=" * 80)

    clear_memory()
    memory_before = get_gpu_memory_mb()

    print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆãƒ­ãƒ¼ãƒ‰å‰ï¼‰: {memory_before:.2f} MB")

    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    print(f"ğŸš€ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­: {model_name}")
    llm = LLM(
        model=model_name,
        gpu_memory_utilization=0.85,
    )

    memory_after = get_gpu_memory_mb()
    memory_reserved = get_gpu_memory_reserved_mb()

    print(f"âœ… ãƒ­ãƒ¼ãƒ‰å®Œäº†")
    print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆãƒ­ãƒ¼ãƒ‰å¾Œï¼‰: {memory_after:.2f} MB")
    print(f"ãƒ¡ãƒ¢ãƒªäºˆç´„é‡: {memory_reserved:.2f} MB")

    base_memory = memory_after - memory_before

    print(f"\nğŸ“ˆ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«æ¶ˆè²»ãƒ¡ãƒ¢ãƒª: {base_memory:.2f} MB")

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    del llm
    clear_memory()

    return {
        "name": "ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿",
        "memory_mb": base_memory,
        "memory_allocated_mb": memory_after,
        "memory_reserved_mb": memory_reserved,
    }


def measure_individual_loras(model_name: str, lora_adapters: List[Dict]) -> List[Dict]:
    """å„LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’å€‹åˆ¥ã«ãƒ­ãƒ¼ãƒ‰ã—ã¦æ¸¬å®š"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¸¬å®š 2/3: å€‹åˆ¥LoRAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆÃ—3ï¼‰")
    print("=" * 80)

    results = []

    for i, adapter in enumerate(lora_adapters, 1):
        print(f"\n--- LoRA {i}/{len(lora_adapters)}: {adapter['name']} ---")

        clear_memory()
        memory_before = get_gpu_memory_mb()

        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆãƒ­ãƒ¼ãƒ‰å‰ï¼‰: {memory_before:.2f} MB")

        # å€‹åˆ¥ã®LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆï¼ˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« + LoRAï¼‰
        print(f"ğŸš€ ãƒ¢ãƒ‡ãƒ« + LoRAåˆæœŸåŒ–ä¸­...")
        llm = LLM(
            model=model_name,
            enable_lora=True,
            max_loras=1,
            max_lora_rank=64,
            gpu_memory_utilization=0.85,
        )

        # LoRAã‚’ä½¿ã£ã¦1å›æ¨è«–ï¼ˆå®Ÿéš›ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã‚ˆã†ã«ï¼‰
        lora_request = LoRARequest(
            lora_name=adapter['name'],
            lora_int_id=i,
            lora_path=adapter['path'],
        )

        print(f"ğŸ”„ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        _ = llm.generate(
            prompts=["<|user|>\nTest</s>\n<|assistant|>\n"],
            lora_request=lora_request,
        )

        memory_after = get_gpu_memory_mb()
        memory_reserved = get_gpu_memory_reserved_mb()

        print(f"âœ… ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆãƒ­ãƒ¼ãƒ‰å¾Œï¼‰: {memory_after:.2f} MB")
        print(f"ãƒ¡ãƒ¢ãƒªäºˆç´„é‡: {memory_reserved:.2f} MB")

        consumed_memory = memory_after - memory_before

        print(f"\nğŸ“ˆ {adapter['name']} æ¶ˆè²»ãƒ¡ãƒ¢ãƒª: {consumed_memory:.2f} MB")

        results.append({
            "name": adapter['name'],
            "description": adapter['description'],
            "memory_mb": consumed_memory,
            "memory_allocated_mb": memory_after,
            "memory_reserved_mb": memory_reserved,
        })

        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del llm
        clear_memory()

    total_individual = sum(r['memory_mb'] for r in results)
    print(f"\nğŸ“Š å€‹åˆ¥LoRAåˆè¨ˆãƒ¡ãƒ¢ãƒª: {total_individual:.2f} MB")

    return results


def measure_multi_lora(model_name: str, lora_adapters: List[Dict]) -> Dict:
    """Multi-LoRA servingï¼ˆ3ã¤åŒæ™‚ãƒ­ãƒ¼ãƒ‰ï¼‰ã®ãƒ¡ãƒ¢ãƒªæ¶ˆè²»é‡ã‚’æ¸¬å®š"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¸¬å®š 3/3: Multi-LoRA servingï¼ˆ3ã¤åŒæ™‚ï¼‰")
    print("=" * 80)

    clear_memory()
    memory_before = get_gpu_memory_mb()

    print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆãƒ­ãƒ¼ãƒ‰å‰ï¼‰: {memory_before:.2f} MB")

    # Multi-LoRAå¯¾å¿œã®LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    print(f"ğŸš€ Multi-LoRAãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
    llm = LLM(
        model=model_name,
        enable_lora=True,
        max_loras=len(lora_adapters),
        max_lora_rank=64,
        max_cpu_loras=len(lora_adapters) * 2,
        gpu_memory_utilization=0.85,
    )

    # å…¨ã¦ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
    print(f"ğŸ”„ {len(lora_adapters)}å€‹ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    for i, adapter in enumerate(lora_adapters, 1):
        lora_request = LoRARequest(
            lora_name=adapter['name'],
            lora_int_id=i,
            lora_path=adapter['path'],
        )

        print(f"  - {adapter['name']} ã‚’ãƒ­ãƒ¼ãƒ‰...")
        _ = llm.generate(
            prompts=["<|user|>\nTest</s>\n<|assistant|>\n"],
            lora_request=lora_request,
        )

    memory_after = get_gpu_memory_mb()
    memory_reserved = get_gpu_memory_reserved_mb()

    print(f"âœ… ãƒ­ãƒ¼ãƒ‰å®Œäº†")
    print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆãƒ­ãƒ¼ãƒ‰å¾Œï¼‰: {memory_after:.2f} MB")
    print(f"ãƒ¡ãƒ¢ãƒªäºˆç´„é‡: {memory_reserved:.2f} MB")

    multi_lora_memory = memory_after - memory_before

    print(f"\nğŸ“ˆ Multi-LoRA æ¶ˆè²»ãƒ¡ãƒ¢ãƒª: {multi_lora_memory:.2f} MB")

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    del llm
    clear_memory()

    return {
        "name": f"Multi-LoRA ({len(lora_adapters)}å€‹åŒæ™‚)",
        "memory_mb": multi_lora_memory,
        "memory_allocated_mb": memory_after,
        "memory_reserved_mb": memory_reserved,
    }


def print_summary(base_result: Dict, individual_results: List[Dict], multi_lora_result: Dict):
    """çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    print("\n" + "=" * 80)
    print("ğŸ“Š ãƒ¡ãƒ¢ãƒªæ¶ˆè²»é‡ã‚µãƒãƒªãƒ¼")
    print("=" * 80)

    # å€‹åˆ¥LoRAã®åˆè¨ˆ
    total_individual = sum(r['memory_mb'] for r in individual_results)

    print(f"\n1ï¸âƒ£  ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿:")
    print(f"    {base_result['memory_mb']:.2f} MB")

    print(f"\n2ï¸âƒ£  å€‹åˆ¥LoRAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆÃ—{len(individual_results)}ï¼‰:")
    for r in individual_results:
        print(f"    - {r['name']}: {r['memory_mb']:.2f} MB")
    print(f"    åˆè¨ˆ: {total_individual:.2f} MB")

    print(f"\n3ï¸âƒ£  Multi-LoRA serving:")
    print(f"    {multi_lora_result['memory_mb']:.2f} MB")

    # ãƒ¡ãƒ¢ãƒªå‰Šæ¸›åŠ¹æœ
    print(f"\n" + "=" * 80)
    print("ğŸ’° ãƒ¡ãƒ¢ãƒªå‰Šæ¸›åŠ¹æœ")
    print("=" * 80)

    memory_saved = total_individual - multi_lora_result['memory_mb']
    reduction_percent = (memory_saved / total_individual) * 100 if total_individual > 0 else 0

    print(f"\nå€‹åˆ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åˆè¨ˆ: {total_individual:.2f} MB")
    print(f"Multi-LoRA serving:    {multi_lora_result['memory_mb']:.2f} MB")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"å‰Šæ¸›é‡:                {memory_saved:.2f} MB")
    print(f"å‰Šæ¸›ç‡:                {reduction_percent:.1f}%")

    if memory_saved > 0:
        print(f"\nâœ… Multi-LoRA servingã«ã‚ˆã‚Š {memory_saved:.2f} MB ({reduction_percent:.1f}%) ã®ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ï¼")
    else:
        print(f"\nâš ï¸  äºˆæƒ³å¤–: Multi-LoRAã®æ–¹ãŒãƒ¡ãƒ¢ãƒªæ¶ˆè²»ãŒå¤šã„ï¼ˆæ¸¬å®šèª¤å·®ã®å¯èƒ½æ€§ï¼‰")

    print(f"\nğŸ’¡ ç†è«–çš„ãªåˆ©ç‚¹:")
    print(f"   - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¯1ã¤ã ã‘ãƒ­ãƒ¼ãƒ‰ï¼ˆå…±æœ‰ï¼‰")
    print(f"   - LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯è»½é‡ï¼ˆæ•°MBï½æ•°ç™¾MBï¼‰")
    print(f"   - å€‹åˆ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã¯ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’{len(individual_results)}å›ãƒ­ãƒ¼ãƒ‰")


def main():
    parser = argparse.ArgumentParser(
        description="Multi-LoRA servingã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›åŠ¹æœã‚’æ¸¬å®š"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        help="ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«",
    )
    parser.add_argument(
        "--output",
        type=Path,
        help="çµæœã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
    )

    args = parser.parse_args()

    if not torch.cuda.is_available():
        print("âŒ ã‚¨ãƒ©ãƒ¼: CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚GPUãŒå¿…è¦ã§ã™ã€‚")
        return

    print("=" * 80)
    print("ğŸ”¬ Multi-LoRA Serving ãƒ¡ãƒ¢ãƒªæ¸¬å®š")
    print("=" * 80)
    print(f"ãƒ¢ãƒ‡ãƒ«: {args.model}")
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPUç·ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024 / 1024:.2f} MB")

    # æ¸¬å®šå®Ÿè¡Œ
    base_result = measure_base_model(args.model)
    individual_results = measure_individual_loras(args.model, LORA_ADAPTERS)
    multi_lora_result = measure_multi_lora(args.model, LORA_ADAPTERS)

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print_summary(base_result, individual_results, multi_lora_result)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)

        total_individual = sum(r['memory_mb'] for r in individual_results)
        memory_saved = total_individual - multi_lora_result['memory_mb']
        reduction_percent = (memory_saved / total_individual) * 100 if total_individual > 0 else 0

        with open(args.output, 'w', encoding='utf-8') as f:
            f.write("Multi-LoRA Serving ãƒ¡ãƒ¢ãƒªæ¸¬å®šçµæœ\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"ãƒ¢ãƒ‡ãƒ«: {args.model}\n")
            f.write(f"GPU: {torch.cuda.get_device_name(0)}\n\n")

            f.write("1. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿\n")
            f.write(f"   {base_result['memory_mb']:.2f} MB\n\n")

            f.write("2. å€‹åˆ¥LoRAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n")
            for r in individual_results:
                f.write(f"   - {r['name']}: {r['memory_mb']:.2f} MB\n")
            f.write(f"   åˆè¨ˆ: {total_individual:.2f} MB\n\n")

            f.write("3. Multi-LoRA serving\n")
            f.write(f"   {multi_lora_result['memory_mb']:.2f} MB\n\n")

            f.write("ãƒ¡ãƒ¢ãƒªå‰Šæ¸›åŠ¹æœ\n")
            f.write("=" * 80 + "\n")
            f.write(f"å‰Šæ¸›é‡: {memory_saved:.2f} MB\n")
            f.write(f"å‰Šæ¸›ç‡: {reduction_percent:.1f}%\n")

        print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜: {args.output}")

    print("\nâœ… ãƒ¡ãƒ¢ãƒªæ¸¬å®šå®Œäº†ï¼")


if __name__ == "__main__":
    main()
